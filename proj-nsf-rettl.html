<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html" charset="utf-8" />
  <title>DM2 Lab: Data Mining towards Decision Making</title>
  <!-- Favicon -->
  <link rel="shortcut icon" href="images/favicon.gif" />
  <!-- Standard reset, fonts and grids -->
  <link rel="stylesheet" type="text/css" href="styles/reset-fonts-grids.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <!-- styles for the whole website -->
  <link href="styles/styles.css" rel="stylesheet" type="text/css" />
  <!-- scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="http://www.francois-petitjean.com/main.js" type="text/javascript"></script>
</head>

<body class="yui-skin-sam" id="yahoo-com">
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

  <div id="doc" class="yui-t1">
  <div id="hd">
    <div id="header"><a href="https://www.nd.edu/"><img id="nd-logo" src="images/logo_nd.png" alt="CSE@NotreDame" /></a></div>
  </div>

  <h1>NSF RETTL: Collaborative Research: Advancing STEM Online Learning by Augmenting Accessibility with Explanatory Captions and AI</h1>
 
  <h2>Project Description (NSF IIS-2119531)</h2>

  <div><b>PI:</b> <a href="https://ischool.illinois.edu/people/yun-huang">Yun Huang</a>; <b>co-PIs:</b> <a href="https://cs.illinois.edu/about/people/faculty/angrave">Lawrence Angrave</a>, <a href="http://www.meng-jiang.com">Meng Jiang</a>, and <a href="https://personnel.gallaudet.edu/personnel/qi-wang/">Qi Wang</a></div>
 
  <br>
  
  <div>Videos are a popular medium for online learning, in which captions are essential for increasing accessibility to students for effective learning. This research identifies two types of video captions: typical closed captions and explanatory captions. Closed captions are a text representation of the spoken part of a video. Explanatory captions are created to give students insights into the visual, textual, and audio content of a video. Existing technologies have focused on automatically generating or improving the quality of closed captions. For STEM learning, explanatory captions have the potential to play a new role in learning. This project will work to devise effective Q/A mechanisms and effective interaction designs that enable students and instructors to generate explanatory captions for STEM videos in a collaborative manner. The proposed technologies will augment accessibility and learning experiences for under-served populations, including the Deaf and Hard-of-Hearing (DHH) community, made up of 48 million Americans, while also improving comprehension for non-native English speakers, even those without hearing impairments. Evaluation sites include both Gallaudet University, the worldâ€™s only liberal arts university dedicated exclusively to educating DHH learners, and the University of Illinois at Urbana-Champaign, which has the largest international student population amongst U.S. public institutions and supports students with disabilities in inclusive learning environments.</div>

  <br>

  <div>This interdisciplinary research draws from and contributes to both computer science and learning science, and accessibility practices in the following areas. The first step is discovering new knowledge about how accessibility-enabled videos (with explanatory and closed captions) broaden the participation of under-served populations in STEM learning. This will provide the foundation for developing a theory of how explanatory captions can contribute to learning and effective mechanisms, based on crowdsourced human contributions and machine learning algorithms, to create these explanatory captions for STEM videos at different learning stages (e.g., preparing, tracking, trouble-shooting, and reflecting). The investigators will then use the theory to create a novel chatbot that enables knowledge sharing for students with diverse backgrounds. Theoretical frameworks--ICAP (interactive, constructive, active, and passive) and Community of Inquiry will guide the evaluation of how explanatory captions and chatbots can contribute to learning. Finally, the team will acquire empirical understanding of how augmented accessibility with AI agents (e.g., chatbots) impacts students' and instructors' practices.</div>

  <h2>Faculty</h2>

    <table>
    <td width="150" height="155">
        <img src="lab/images/meng.jpg" height="150" />
    </td>
    <td width="600">
        <div><a href="http://www.meng-jiang.com">Meng Jiang</a></div>
        <div>Email: mjiang2@nd.edu</div>
    </td>
    </table>

  <h2>Graduate Student</h2>

    <table>
    <td width="150" height="155">
        <img src="lab/images/mengxia.jpg" height="150" />
    </td>
    <td width="850">
        <div><a href="#">Mengxia Yu</a>: PhD student (2020-); CSE Select Fellowship</div>
        <div>Email: myu2 [at] nd.edu</div>
    </td>
    </table>

    <table>
    <td width="150" height="155">
        <img src="lab/images/wenhao.jpg" height="150" />
    </td>
    <td width="850">
        <div><a href="https://wyu97.github.io/">Wenhao Yu</a>: PhD student (2019-)</div>
        <div>Email: wyu1 [at] nd.edu</div>
    </td>
    </table>

  <h2>Publications</h2>

  <ul>

    <li class="O"><a href="#">Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts</a>
    by W. Yu, C. Zhu, L. Qin, T. Zhao, M. Jiang.
    Findings of <i>Annual Meeting of the Association for Computational Linguistics (<b>ACL</b>)</i>, 2022.
    </li>

    <li class="O"><a href="#">Dict-BERT: Enhancing Language Model Pre-training with Dictionary</a>
    by W. Yu, C. Zhu, Y. Fang, D. Yu, S. Wang, Y. Xu, M. Zeng, M. Jiang.
    Findings of <i>Annual Meeting of the Association for Computational Linguistics (<b>ACL</b>)</i>, 2022.
    </li>

  </ul>

  <br><br><br><br><br>
  <br><br><br><br><br>

    <table>
    <td width="100" height="100">
        <img src="images/nsf.jpg" width="100" />
    </td>
    <td width="100" height="100">
        <img src="images/ndengineering.png" width="100" />
    </td>
    </table>
  </div>

  <br><br><br><br><br>  
  
</body>

